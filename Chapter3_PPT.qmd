---
title: "Interval Estimation and Hypothesis Testing (Chap 3)"
format: revealjs
editor: visual
---


## Interval Estimation

- Interval estimation proposes a range of values in which the true parameter $\beta$ is likely to fall. 
- Providing a range of values gives a sense of what the parameter value might be, and the precision with which we have estimated it. 
- Such intervals are often called **confidence intervals**, though we prefer to call them **interval estimates** because the term “confidence” is widely misunderstood and misused.

---

## The t-Distribution

- The normal distribution of $b_2$, the least squares estimator of $\beta_2$, is:

$$b_2 | x \sim N \left( \beta_2, \frac{\sigma^2}{\sum (x_i - \bar{x})^2} \right)$$
- Standardize $b_2$, we get:
$$Z = \frac{b_2 - \beta_2}{\sqrt{\sigma^2 / \sum (x_i - \bar{x})^2}} \sim N(0,1)$$

---

## The t-Distribution ...

- We know that, if $\alpha=5\%$:

 $$P(-1.96 \leq Z \leq 1.96) = 1-0.05 =  0.95$$

- Substituting: $$ P\left(-1.96 \leq \frac{b_2 - \beta_2}{\sqrt{\sigma^2 / \sum (x_i - \bar{x})^2}} \leq 1.96\right) = 0.95$$
 
--- 


## The t-Distribution ...

- Rearranging:

$$P\left(b_2 - 1.96\sqrt{\frac{\sigma^2}{\sum (x_i - \bar{x})^2}}   \leq \beta_2 \leq b_2 + 1.96\sqrt{\frac{\sigma^2}{\sum (x_i - \bar{x})^2}}  \right) = 0.95$$


---

## The t-Distribution...

 - The two endpoints 
 $$b_2 \pm 1.96\sqrt{\sigma^2/\sum(x_i - \overline{x})^2}$$ 
provide an interval estimator. 

- In repeated sampling, 95% of the intervals constructed this way will contain the true value of the parameter $\beta_2$.

- This derivation assumes **SR6** and that we know the variance of the error term $\sigma^2$.

---

## The t-Distribution...

- Replacing $\sigma^2$ with $\hat{\sigma}^2$ creates a random variable $t$:

$$t = \frac{b_2 - \beta_2}{\sqrt{\hat{\sigma}^2 / \sum (x_i - \bar{x})^2}} = \frac{b_2 - \beta_2}{\sqrt{v \hat{a} r (b_2)}} = \frac{b_2 - \beta_2}{se(b_2)} \sim t_{(N-2)}$$

- This substitution changes the probability distribution from standard normal to a t-distribution with $N - 2$ degrees of freedom. 

- We denote this as $t \sim t_{(N-2)}$

---

## The t-Distribution...

- The $t$-distribution is a bell-shaped curve centered at zero. 

- It looks like the standard normal distribution but is more spread out, with a larger variance and thicker tails. 

- The shape is controlled by a single parameter called the **degrees of freedom** ($df$).

---

```{r normal-vs-t-distribution, echo=FALSE, fig.width=10, fig.height=5}
# Load required libraries
library(ggplot2)
library(patchwork)
library(dplyr)

# Create data for comparison
x <- seq(-4, 4, length.out = 1000)

# Standard normal distribution
normal_data <- data.frame(x = x, y = dnorm(x), Distribution = "Standard Normal")

# t-distributions with different degrees of freedom
t_data_5 <- data.frame(x = x, y = dt(x, df = 5), Distribution = "t (df = 5)")
t_data_10 <- data.frame(x = x, y = dt(x, df = 10), Distribution = "t (df = 10)")
t_data_30 <- data.frame(x = x, y = dt(x, df = 30), Distribution = "t (df = 30)")

# Combine all data
all_data <- rbind(normal_data, t_data_5, t_data_10, t_data_30)

# Create individual plots
plot_normal <- ggplot(normal_data, aes(x = x, y = y)) +
  geom_line(color = "blue", linewidth = 1.2) +
  geom_area(fill = "blue", alpha = 0.2) +
  labs(title = "Standard Normal Distribution (N(0,1))",
       x = "z-value", y = "Density") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()) +
  xlim(-4, 4)

plot_t_comparison <- ggplot(all_data, aes(x = x, y = y, color = Distribution, linetype = Distribution)) +
  geom_line(linewidth = 1.2) +
  labs(title = "Comparison with t-Distributions",
       x = "t-value", y = "Density",
       color = "Distribution", linetype = "Distribution") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        legend.position = "bottom",
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()) +
  scale_color_manual(values = c("Standard Normal" = "blue", 
                               "t (df = 5)" = "red", 
                               "t (df = 10)" = "green", 
                               "t (df = 30)" = "purple")) +
  scale_linetype_manual(values = c("Standard Normal" = "solid",
                                  "t (df = 5)" = "dashed",
                                  "t (df = 10)" = "dotdash",
                                  "t (df = 30)" = "dotted")) +
  xlim(-4, 4)

# Combine plots side by side
combined_plot <- plot_normal + plot_t_comparison +
  plot_annotation(title = "Comparison of t-Distributions with Standard Normal Distribution",
                  theme = theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold")))

# Display the combined plot
combined_plot
```

---

```{r t-distribution-two-tailed, echo=FALSE, fig.align='center'}
# Load required library
library(ggplot2)

# Create data for t-distribution
x <- seq(-4, 4, length.out = 1000)
df <- 10  # Degrees of freedom (m)
y <- dt(x, df)

# Critical values for two-tailed test (alpha = 0.05)
alpha <- 0.05
critical_left <- qt(alpha/2, df)
critical_right <- qt(1 - alpha/2, df)

# Create data frame
plot_data <- data.frame(x = x, y = y)

# Create the plot
ggplot(plot_data, aes(x = x, y = y)) +
  geom_line(linewidth = 1) +
  
  # Add x-axis line
  geom_hline(yintercept = 0, color = "black", linewidth = 0.5) +
  
  # Shade the left tail rejection region
  geom_area(data = subset(plot_data, x <= critical_left), 
            aes(x = x, y = y), fill = "red", alpha = 0.3) +
  
  # Shade the right tail rejection region
  geom_area(data = subset(plot_data, x >= critical_right), 
            aes(x = x, y = y), fill = "red", alpha = 0.3) +
  
  # Add critical value lines (segmented lines that stop at x-axis)
  geom_segment(x = critical_left, xend = critical_left, 
               y = 0, yend = dt(critical_left, df), 
               linetype = "dashed", color = "blue") +
  geom_segment(x = critical_right, xend = critical_right, 
               y = 0, yend = dt(critical_right, df), 
               linetype = "dashed", color = "blue") +
  
  # Add critical value labels on x-axis with t_c notation
  annotate("text", x = critical_left, y = -0.02, 
           label = expression(t[c] == -t[(alpha/2 * "," ~ m)]), 
           parse = TRUE, color = "blue", size = 5, vjust = 1) +
  annotate("text", x = critical_right, y = -0.02, 
           label = expression(t[c] == t[(1-alpha/2 * "," ~ m)]), 
           parse = TRUE, color = "blue", size = 5, vjust = 1) +
  
  # Add alpha/2 labels in the tails
  annotate("text", x = critical_left - 0.5, y = 0.05, 
           label = expression(alpha/2), 
           parse = TRUE, color = "red", size = 6) +
  annotate("text", x = critical_right + 0.5, y = 0.05, 
           label = expression(alpha/2), 
           parse = TRUE, color = "red", size = 6) +
  
  # Add center region label (1-alpha)
  annotate("text", x = 0, y = 0.2, 
           label = expression(1 - alpha), 
           parse = TRUE, color = "black", size = 6) +
  
  # Add arrows showing the area in each tail
  annotate("segment", x = critical_left - 0.3, xend = critical_left - 0.3, 
           y = 0.04, yend = 0.01, color = "red", 
           arrow = arrow(length = unit(0.2, "cm"), type = "closed")) +
  annotate("segment", x = critical_right + 0.3, xend = critical_right + 0.3, 
           y = 0.04, yend = 0.01, color = "red", 
           arrow = arrow(length = unit(0.2, "cm"), type = "closed")) +
  
  # Customize the plot appearance
  labs(x = "", y = "") +
  theme_minimal() +
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    panel.grid = element_blank(),
    axis.title.x = element_text(size = 12),
    axis.title.y = element_text(size = 12),
    plot.caption = element_text(hjust = 0.5)
  ) +
  scale_x_continuous(breaks = NULL) +
  ylim(-0.03, max(y) * 1.1)
```

- We can find a critical value $t_c$ from a t-distribution so that:

$$P(t \geq t_c) = P(t \leq -t_c) = \alpha / 2, \qquad P(-t_c \leq t \leq t_c) = 1 - \alpha$$

- $\alpha$ is a probability often taken to be $\alpha = 1\%$ or $\alpha = 5\%$.


---


## The t-Distribution ...

- Substituting for $t$,

$$P\left( -t_c \leq \frac{b_k - \beta_k}{se(b_k)} \leq t_c \right) = 1 - \alpha$$

Or,

$$P[b_k - t_c se(b_k) \leq \beta_k \leq b_k + t_c se(b_k)] = 1 - \alpha$$

---

## Obtaining Interval Estimates...

- When $b_k$ and $\text{se}(b_k)$ are estimated values based on a given sample of data, then

   $$b_k \pm t_c \text{se}(b_k)$$

is called a $100(1-\alpha)$% interval estimate of $\beta_k$, 

- Or, equivalently, a $100(1-\alpha)$% confidence interval.

- Usually $\alpha = 1\%$ or $\alpha = 5\%$, so we obtain a 99% or 95% confidence interval.

---

## Example 3.1: Interval estimates of food expenditure data

```{r}
# Load necessary libraries and data
rm(list=ls())
library(mosaic)

# Load food expenditure data
load(url("http://www.principlesofeconometrics.com/poe5/data/rdata/food.rdata"))

```


```{r,echo=TRUE}
# Estimate simple linear regression model
fit <- lm(food_exp ~ income, data = food)
summary(fit)
```

---

## Example 3.1...

- Total observation (N) = 40,

- the degree of freedom (df) are $N-2=38$

- For a $95\%$ confidence interval, $\alpha = 0.05$

- The critical value $t_c = t_{(1-\alpha/2,N-2)}= t_{(0.975,38)}= 2.024$ is \newline
  the $97.5$ percentile from the t-distribution with df$=38$ 

- The interval estimate for $\beta_2$ is: $$b_2 \pm t_c \text{se}(b_2)=b_2 \pm 2.024*se(b_2)$$


---

## Variance/standard error of the paramters

```{r,echo=TRUE}
# Variance-covariance matrix 
vcov(fit)

# standard errors of the parameters
sqrt(diag(vcov(fit)))

```

- The $se(b_2)=2.09$ 

---

- Stubs.the values $b_2 = 10.21$ & $se(b_2)=2.09$, \newline
the interval estimate for $\beta_2$: 

$$10.21 \pm (2.024) *(2.09)= \left[ 5.97, 14.45\right]$$

- That is, we estimate "with $95\%$ confidence" that from additional $\$100$ of weekly income, hhs will spend b/n $\$ 5.97$ & $\$ 14.45$ on food

---

```{r,echo=TRUE}

# Manual 95% CI for slope
coef(fit)[2] - qt(0.975, 38) * sqrt(diag(vcov(fit)))[2] # lower limit 
coef(fit)[2] + qt(0.975, 38) * sqrt(diag(vcov(fit)))[2] # upper limit 
# Using the mosaic function 
confint(fit)

confint(fit, level = 0.95)  # 90% CI
```

---

## Notes that...

 The interpretation of confidence intervals requires care.

  - The properties are based on repeated sampling. 

  - Any one interval estimate may or may not contain the true parameter $\beta_k$, and since $\beta_k$ is unknown, we never know. 

  - Our confidence is in the procedure used to construct the interval estimate, not in any one interval calculated from a sample.

---

## The Sampling Variation 

```{r,echo=TRUE}
#' Bootstrap a 10 regressions
table <- do(10)*lm(food_exp~income, data=resample(food))

```

```{r}
# to get the standard error of the parameter estimates
library(tidyverse)
library(broom)

results <- do(10) * tidy(lm(food_exp ~ income, data = resample(food)))

wide_results <- results %>%
  select(.index, term, estimate, std.error) %>%
  pivot_wider(
    names_from = term,
    values_from = c(estimate, std.error)
  )

# All the relevant output together 
df <- cbind(wide_results, sigma =table[,3]) %>% 
  rename(Sample = .index, b1= `estimate_(Intercept)`, 'se(b1)' = `std.error_(Intercept)`, b2 = estimate_income, 
         'se(b2)'= std.error_income) %>% select(Sample, b1,'se(b1)',b2, 'se(b2)',sigma) %>% 
  mutate(across(where(is.numeric), ~ signif(.x, 4)))   # round all numeric columns to 2 significant digits
df
```



---

- The $95\%$ confidence intervals for $\beta_1$ and $\beta_2$ are: 

```{r}
CI <- df %>%
  mutate(
    `b1-tc*se(b1)` = b1 - qt(0.975, 38) * `se(b1)`,
    `b1+tc*se(b1)` = b1 + qt(0.975, 38) * `se(b1)`,
    `b2-tc*se(b2)` = b2 - qt(0.975, 38) * `se(b2)`,
    `b2+tc*se(b2)` = b2 + qt(0.975, 38) * `se(b2)`
  ) %>%
  select(Sample, `b1-tc*se(b1)`, `b1+tc*se(b1)`, `b2-tc*se(b2)`, `b2+tc*se(b2)`) %>% 
  mutate(across(where(is.numeric), ~ signif(.x, 4)))   # round all numeric columns to 2 significant digits

CI
```

- **Question:** How many of the these intervals \newline
contain the true parameters, and which ones are they?


---

  > Answer: We do not know. 

- But $95\%$ of all interval constructed this way contain the true parameter values. 

---

Recap: 

- Sampling variability causes the:

  - center of each of the interval estimates to change with the values of the least squares estimates
  - The widths of the intervals to change with the standard errors


- Interval estimators are a convenient way to report regression results. 

- Interval estimators combine point estimation with a measure of sampling variability 



---

## Hypothesis Tests

- Hypothesis testing is a procedure for comparing a claim about a population with the evidence provided by a sample of data.

**Example (Food Expenditure):**

  - Suppose we want to know whether $\beta_2$ (the effect of income on food expenditure) is greater than $10$.

  - This matters for decision-making: if $\beta_2 >10$, a $\$100$ increase in income increases food spending by more than $\$10$.

  - Economic theory also suggests that $\beta_2$ should be positive.

**Goal:** Use the sample data to check whether the theoretical proposition (e.g., $\beta_2 >10$  is supported.

---

## Hypothesis Tests...

Every hypothesis test has five ingredients:

i. A null hypothesis $H_0$

ii. An alternative hypothesis $H_1$

iii. A test statistic

iv. A rejection region

v. A conclusion

---

## i) The Null Hypothesis

- The null hypothesis $H_0$ specifies a value for a regression parameter.

- That is,  $\beta_k$ for $k = 1$ or $2$.

- It is stated as $H_0: \beta_k = c$, where $c$ is a constant.

- A null hypothesis is the belief we will maintain until we are convinced by the sample evidence that it is not true, in which case we reject the $𝐻_0$. 


---

## ii) The Alternative Hypothesis 

- Paired with every $H_0$ is a logical alternative hypothesis $𝐻_1$ that we will accept if the $𝐻_0$ is rejected

- The alternative hypothesis $H_1$ is accepted if the null is rejected. 

- For $H_0: \beta_k = c$, there are three possible alternative hypotheses.

  1. $H_1: \beta_k > c$, $\implies$ accept the conclusion $\beta_k > c$
  
  2. $H_1: \beta_k < c$,$\implies$ accept the conclusion $\beta_k < c$
  
  3. $H_1: \beta_k \neq c$, $\beta_k$ takes a value  either larger/smaller than $c$
  
  

  
---

## iii) The Test Statistic

- The sample information about the $H_0$ is embodied in the sample value of a test statistic


- A test statistics has a special characteristic: 

   - Its probability distribution is completely known when the$H_0$ is true
   - It has some other distribution if the $H_0$ is not true

---

## iii) The Test ...

- It all starts with:
$$t=\frac{b_k-\beta_k}{se(b_k)}$$
- If $H_0:\beta_k = c$ is true, then, we can substitute $c$ for $\beta_k$ and it follows that


  $$t=\frac{b_k-c}{se(b_k)} \sim t_{(N-2)}$$

---

## iv) The rejection Region

- The rejection region depends on the $H_1$ 
- It is the range of values of the test statistic that leads to rejection of $H_0$ 

- The rejection region consists of values that are **unlikely** and that have low probability of occurring when the $H_0$ is true


---

## iv) The rejection...

- It is possible to construct a rejection region only if we have:

   - A test statistic whose distribution is known when the $H_0$ is true

   - An alternative hypothesis, $H_1$

   - A level of significance, $\alpha$ 

- The level of significance $\alpha$ is usually $1\%, 5\%$ or $10\%$

---

## iv) The rejection...

- If we reject the $H_0$ when it is true, then we commit what is called a **Type I error**

 > we can specify the amount of Type I error we will tolerate by setting the level of significance α

- If we do not reject a $H_0$ that is false, then we have committed a **Type II error**

> we cannot control or calculate the probability of this type of error


---

## v) Conclusion

- When you have completed testing a hypothesis, you should state your conclusion

- Do you reject/not reject the $H_0$?

- Interpret the conclusion in economic context

- Avoid saying that you “accept” the $H_0$, which can be very misleading

---

## Rejection Regions for Specific Alternatives

- To have a rejection region for a null hypothesis, We need

  1. a test statistic
  
  2. a specific alternative, $\beta_k > c, \ \beta_k <c$ and $\beta_k \neq c$
  
  3. to specify the level of significance of the test

---

## Rejection region with  Alternative ">"

- $H_0: \beta_k = c$ vs $H_1: \beta_k > c$ 

- Reject $H_0$ if $t \geq t_{(1-\alpha, N-2)}$

- This is a one-tail hypothesis/test

```{r}
# Load required library
library(ggplot2)

# Create data for the normal distribution
x <- seq(-3, 3, length.out = 1000)
y <- dnorm(x)

# Critical value for one-tailed test (alpha = 0.05)
critical_value <- qnorm(0.95)

# Create data frame
plot_data <- data.frame(x = x, y = y)

# Create the plot
ggplot(plot_data, aes(x = x, y = y)) +
  geom_line(linewidth = 1) +
  
  # Add x-axis line
  geom_hline(yintercept = 0, color = "black", linewidth = 0.5) +
  
  # Shade the rejection region
  geom_area(data = subset(plot_data, x >= critical_value), 
            aes(x = x, y = y), fill = "red", alpha = 0.3) +
  
  # Add critical value line (segmented line that stops at x-axis)
  geom_segment(x = critical_value, xend = critical_value, 
               y = 0, yend = max(y), 
               linetype = "dashed", color = "blue") +
  
  # Add labels and annotations
  annotate("text", x = critical_value, y = -0.05, 
           label = expression(t[c] == t[(1-alpha * "," ~ N-2)]), 
           parse = TRUE, color = "blue", size = 5, vjust = 1) +
  
  # Text in acceptance region (moved further left - about 1cm equivalent)
  annotate("text", x = critical_value/2 - 0.5, y = 0.2, 
           label = "Do not reject", color = "black", size = 5) +
  annotate("text", x = critical_value/2 - 0.5, y = 0.15, 
           label = expression(H[0]: beta[k] == c), color = "black", size = 5) +
  
  # Text in rejection region
  annotate("text", x = critical_value + 0.8, y = 0.2, 
           label = "Reject", color = "red", size = 5) +
  annotate("text", x = critical_value + 0.8, y = 0.15, 
           label = expression(H[0]: beta[k] == c), color = "red", size = 5) +
  
  # Alpha level inside the rejection region with arrow showing area
  annotate("text", x = critical_value + 0.6, y = 0.08, 
           label = "", parse = TRUE, color = "red", size = 6) +
  annotate("segment", x = critical_value + 0.6, xend = critical_value + 0.6, 
           y = 0.06, yend = 0.01, color = "red", 
           arrow = arrow(length = unit(0.2, "cm"), type = "closed")) +
  
  # Zero point
  annotate("text", x = 0, y = -0.02, label = "0", size = 5) +
  
  # Customize the plot appearance
  labs(x = "", y = "") +
  theme_minimal() +
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    panel.grid = element_blank(),
    axis.title.x = element_text(size = 12),
    plot.caption = element_text(hjust = 0.5)
  ) +
  scale_x_continuous(breaks = NULL) +
  ylim(-0.06, max(y))
```

---

## Rejection region with  Alternative "<"

- $H_0:\beta =c$ vs $H_1:\beta < c$ 

- Reject $H_0$ if $t \leq t_{(\alpha, N-2)}$

- This is a one-tail hypothesis

```{r }
# Load required library
library(ggplot2)

# Create data for the normal distribution
x <- seq(-3, 3, length.out = 1000)
y <- dnorm(x)

# Critical value for left-tailed test (alpha = 0.05)
critical_value <- qnorm(0.05)

# Create data frame
plot_data <- data.frame(x = x, y = y)

# Create the plot
ggplot(plot_data, aes(x = x, y = y)) +
  geom_line(linewidth = 1) +
  
  # Add x-axis line
  geom_hline(yintercept = 0, color = "black", linewidth = 0.5) +
  
  # Shade the rejection region (left tail)
  geom_area(data = subset(plot_data, x <= critical_value), 
            aes(x = x, y = y), fill = "red", alpha = 0.3) +
  
  # Add critical value line (segmented line that stops at x-axis)
  geom_segment(x = critical_value, xend = critical_value, 
               y = 0, yend = max(y), 
               linetype = "dashed", color = "blue") +
  
  # Add labels and annotations
  annotate("text", x = critical_value, y = -0.05, 
           label = expression(-t[c] == t[(alpha * "," ~ N-2)]), 
           parse = TRUE, color = "blue", size = 5, vjust = 1) +
  
  # Text in acceptance region (moved toward center)
  annotate("text", x = 0.0, y = 0.2, 
           label = "Do not reject", color = "black", size = 5) +
  annotate("text", x = 0.0, y = 0.15, 
           label = expression(H[0]: beta[k] == c), color = "black", size = 5) +
  
  # Text in rejection region (left of critical value)
  annotate("text", x = critical_value - 0.8, y = 0.2, 
           label = "Reject", color = "red", size = 5) +
  annotate("text", x = critical_value - 0.8, y = 0.15, 
           label = expression(H[0]: beta[k] == c), color = "red", size = 5) +
  
  # Alpha level inside the rejection region with arrow showing area
  annotate("text", x = critical_value - 0.6, y = 0.08, 
           label = expression(alpha), parse = TRUE, color = "red", size = 6) +
  annotate("segment", x = critical_value - 0.6, xend = critical_value - 0.6, 
           y = 0.06, yend = 0.01, color = "red", 
           arrow = arrow(length = unit(0.2, "cm"), type = "closed")) +
  
  # Zero point
  annotate("text", x = 0, y = -0.02, label = "0", size = 5) +
  
  # Customize the plot appearance
  labs(x = "", y = "") +
  theme_minimal() +
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    panel.grid = element_blank(),
    axis.title.x = element_text(size = 12),
    plot.caption = element_text(hjust = 0.5)
  ) +
  scale_x_continuous(breaks = NULL) +
  ylim(-0.06, max(y))
```

---

## Rejection region with alternative "≠"

- $H_0:\beta =c$ vs $H_1:\beta \neq c$ 

- Reject $H_0$ if $t \leq t_{(\alpha/2, N-2)}$ or $t \geq t_{(1-\alpha/2, N-2)}$

- This is a two-tail hypothesis

```{r}
# Load required library
library(ggplot2)

# Create data for the normal distribution
x <- seq(-3, 3, length.out = 1000)
y <- dnorm(x)

# Critical values for two-tailed test (alpha = 0.05, so alpha/2 = 0.025 in each tail)
critical_value_left <- qnorm(0.025)
critical_value_right <- qnorm(0.975)

# Create data frame
plot_data <- data.frame(x = x, y = y)

# Create the plot
ggplot(plot_data, aes(x = x, y = y)) +
  geom_line(linewidth = 1) +
  
  # Add x-axis line
  geom_hline(yintercept = 0, color = "black", linewidth = 0.5) +
  
  # Shade the rejection regions (both tails)
  geom_area(data = subset(plot_data, x <= critical_value_left), 
            aes(x = x, y = y), fill = "red", alpha = 0.3) +
  geom_area(data = subset(plot_data, x >= critical_value_right), 
            aes(x = x, y = y), fill = "red", alpha = 0.3) +
  
  # Add critical value lines (segmented lines that stop at x-axis)
  geom_segment(x = critical_value_left, xend = critical_value_left, 
               y = 0, yend = max(y), 
               linetype = "dashed", color = "blue") +
  geom_segment(x = critical_value_right, xend = critical_value_right, 
               y = 0, yend = max(y), 
               linetype = "dashed", color = "blue") +
  
  # Add labels and annotations for critical values
  annotate("text", x = critical_value_left, y = -0.05, 
           label = expression(-t[c] == t[(alpha/2 * "," ~ N-2)]), 
           parse = TRUE, color = "blue", size = 5, vjust = 1) +
  annotate("text", x = critical_value_right, y = -0.05, 
           label = expression(t[c] == t[(1-alpha/2 * "," ~ N-2)]), 
           parse = TRUE, color = "blue", size = 5, vjust = 1) +
  
  # Text in acceptance region (center)
  annotate("text", x = 0, y = 0.2, 
           label = "Do not reject", color = "black", size = 5) +
  annotate("text", x = 0, y = 0.15, 
           label = expression(H[0]: beta[k] == c), color = "black", size = 5) +
  
  # Text in left rejection region
  annotate("text", x = critical_value_left - 0.8, y = 0.2, 
           label = "Reject", color = "red", size = 5) +
  annotate("text", x = critical_value_left - 0.8, y = 0.15, 
           label = expression(H[0]), color = "red", size = 5) +
  
  # Text in right rejection region
  annotate("text", x = critical_value_right + 0.8, y = 0.2, 
           label = "Reject", color = "red", size = 5) +
  annotate("text", x = critical_value_right + 0.8, y = 0.15, 
           label = expression(H[0]), color = "red", size = 5) +
  
  # Alpha level inside the rejection regions with arrows showing area
  annotate("text", x = critical_value_left - 0.6, y = 0.08, 
           label = expression(alpha/2), parse = TRUE, color = "red", size = 6) +
  annotate("segment", x = critical_value_left - 0.6, xend = critical_value_left - 0.6, 
           y = 0.06, yend = 0.01, color = "red", 
           arrow = arrow(length = unit(0.2, "cm"), type = "closed")) +
  
  annotate("text", x = critical_value_right + 0.6, y = 0.08, 
           label = expression(alpha/2), parse = TRUE, color = "red", size = 6) +
  annotate("segment", x = critical_value_right + 0.6, xend = critical_value_right + 0.6, 
           y = 0.06, yend = 0.01, color = "red", 
           arrow = arrow(length = unit(0.2, "cm"), type = "closed")) +
  
  # Zero point
  annotate("text", x = 0, y = -0.02, label = "0", size = 5) +
  
  # Customize the plot appearance
  labs(x = expression(italic(l)(m)), y = "") +
  theme_minimal() +
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    panel.grid = element_blank(),
    axis.title.x = element_text(size = 12),
    plot.caption = element_text(hjust = 0.5)
  ) +
  scale_x_continuous(breaks = NULL) +
  ylim(-0.06, max(y))
```

---

## Step-by-step procedure for testing hypotheses

 1. Determine the $H_0$ & $H_1$

 2. Specify the test statistic & its distribution if the $H_0$ is true

 3. Select $\alpha$ & determine the rejection region

 4. Calculate the sample value of the test statistic

 5. State your conclusion


---

**Example 3.2:** Right-Tail Test of Significance

$H_0: \beta_2 = 0$, $H_1: \beta_2 > 0$

- In this case $c=0$

- The test statistic is $t= \frac{b2-c}{se(b2)} \sim t_{(1-\alpha, N-2)}$

- select $\alpha = 5\%$. The critical value $t_c = t_{(1-0.05, 40-2)} = t(0.95,38)=1.686$

- Thus we will reject the null hypothesis $H_0$ if the calculated value of $t ≥ 1.686$

- If t < 1.686, we will not reject the null hypothesis


  
---

**Example 3.2 **...

- Using the food expenditure data, we found that $b_2 = 10.21$ with standard error $se(b2) = 2.09$

- The value of the test statistic is: $t= \frac{b2-c}{se(b2)} = \frac{10.21-0}{2.09}=4.88$

- The conclusion:
  - Since $t = 4.88 > 1.686$, we **reject** the null hypothesis that  $\beta_2 = 0$ and accept the alternative that $\beta_2 > 0$

---

**Example 3.2 **...

That is, we reject the hypothesis that there is no relationship between income and food expenditure, and conclude that there is a statistically significant positive relationship between household income and food expenditure


---

```{r,echo=TRUE}
library(multcomp)
summary(glht(fit, linfct = c("income = 0")))

```

```{r,echo=TRUE}
# The critical value: 
qt(0.95, 38)  # Critical value for alpha=0.05

```


```{r,echo=TRUE}
# Comparison:  Check if t > critical value
summary(glht(fit, linfct = c("income <= 0")))$test$tstat > qt(0.95, 38)
```


---

## Example 3.3: Test of Economic Hypothesis

$H_0: \beta_2 \leq 5.5$, $H_1: \beta_2 > 5.5$

- In this case $c=5.5$

- The test statistic is $t= \frac{b2-c}{se(b2)} \sim t_{(1-\alpha, N-2)}$

- select $\alpha = 1\%$. The critical value $t_c = t_{(1-0.01, 40-2)} = t(0.99,38)=2.429$

- Thus we will reject the null hypothesis $H_0$ if the calculated value of $t ≥ 2.429$

- If t < 2.429, we will not reject the null hypothesis


---

**Example 3.3 **...

- Again, using the food expenditure data, we found that $b_2 = 10.21$ with standard error $se(b2) = 2.09$

- The value of the test statistic is: $t= \frac{b2-c}{se(b2)} = \frac{10.21-5.5}{2.09}=2.25$

- The conclusion:
  - Since $t = 2.25 < 2.429$, we **do not reject** the $H_0$ that  $\beta_2 \leq 5.5$ and accept the alternative that $\beta_2 > 5.5$

- That is, We are not able to conclude that the new supermarket will be profitable and will not begin construction


---


```{r,echo=TRUE}
# Example 3.3
summary(glht(fit, linfct = c("income <= 5.5")))
# Check if t > critical value
summary(glht(fit, linfct = c("income <= 5.5")))$test$tstat > qt(0.99, 38)
```

---

## Example 3.4: Test of Economic Hypothesis

$H_0: \beta_2 \geq 15$, $H_1: \beta_2 < 15$

- In this case $c=15$

- The test statistic is $t= \frac{b2-c}{se(b2)} \sim t_{(1-\alpha, N-2)}$

- select $\alpha = 5\%$. The critical value $t_c = t_{(0.05, 40-2)} = t(0.05,38)= - 1.686$

- Thus we will reject the null hypothesis $H_0$ if the calculated value of $t \leq - 1.686$

- If $t > - 1.686$, we will not reject the $H_0$


---

**Example 3.4 **...

- Again, using the food expenditure data, we found that $b_2 = 10.21$ with standard error $se(b2) = 2.09$

- The value of the test statistic is: $t= \frac{b2-c}{se(b2)} = \frac{10.21-15}{2.09}= -2.29$

- The conclusion:

  - Since $t = -2.29 < -1.686$, we **reject** the null hypothesis that  $\beta_2 \leq 15$ and accept the alternative that $\beta_2 > 15$

- We conclude that households spend less than $\$ 15$ from each additional $\$100$ income on food. 


---


```{r,echo=TRUE}
# Example 3.4
summary(glht(fit, linfct = c("income >= 15")))
# Check if t < critical value
summary(glht(fit, linfct = c("income >= 15")))$test$tstat < qt(0.05, 38)
```

---

## Example 3.5: Two-Tail Test of Economic Hypothesis

$H_0: \beta_2 = 7.5$, $H_1: \beta_2 \neq 7.5$

- In this case $c=7.5$

- The test statistic is $t= \frac{b2-c}{se(b2)} \sim t_{(1-\alpha, N-2)}$

- select $\alpha = 5\%$. The critical value $t_c = t_{(1-0.025, 40-2)} = t(0.975,38)= - 2.024$

- Thus we will reject the null hypothesis $H_0$ if the calculated value of $t \ge 2.024$ or if $t \leq- 2.024$

- If $-2.024 \leq t \leq 2.024$, we will not reject the null hypothesis


---

**Example 3.5 **...

- Again, using the food expenditure data, we found that $b_2 = 10.21$ with standard error $se(b2) = 2.09$

- The value of the test statistic is: $t= \frac{b2-c}{se(b2)} = \frac{10.21-7.5}{2.09}= 1.29$

- The conclusion:

  - Since $t = -2.204 < t=1.29 \leq 2.204$, we **do not reject** the null hypothesis that  $\beta_2 =7.5$.

- The sample data are consistent with the conjecture households will spend an additional $\$ 7.50$ per additional $\$100$ income on food.  


---

```{r,echo=TRUE}
# Example 3.5
summary(glht(fit, linfct = c("income = 7.5")))
qt(c(0.025, 0.975), 38)  # Critical values for alpha=0.05
# Check if |t| > critical value
abs(summary(glht(fit, linfct = c("income = 7.5")))$test$tstat) > qt(0.975, 38)
```

---

## Example 3.6: Two-Tail Test of Significance

$H_0: \beta_2 = 0$, $H_1: \beta_2 \neq 0$

- In this case $c=0$

- The test statistic is $t= \frac{b2-c}{se(b2)} \sim t_{(1-\alpha/2, N-2)}$

- select $\alpha = 5\%$. The critical value $t_c = t_{(1-0.025, 40-2)} = t(0.975,38)= 2.024$

- Reject $H_0$ if  $t \ge 2.024$ or $t \leq- 2.024$

-  Do not reject $H_0$ if $-2.024 \leq t \leq 2.024$


---

**Example 3.6 **...

- Using the food expenditure data, we have $b_2 = 10.21$  & $se(b2) = 2.09$

- The value of the test statistic is: $t= \frac{b2-c}{se(b2)} = \frac{10.21-0}{2.09}= 4.88$

- The conclusion:

  - Since $t = 4.88 > 2.204$, we **reject** the null hypothesis that  $\beta_2 =0$.

- We conclude that there is a statistically significant relationship between income and food expenditure.  


---



```{r,echo=TRUE}
# Example 3.6
summary(fit)  # Includes two-tail test for slope
```

--- 

## CI approach of testing two-tail hypothesis

### Decision Rule

- If $0$ is inside the confidence interval, you fail to reject $H_0$.

- If $0$ is not inside the confidence interval, you reject $H_0$ at significance level $\alpha$.

```{r,echo=TRUE}
# Example 3.6
confint(fit, level=0.95)  # CI for slope
```

---
  
## The p-Value

- When reporting the outcome of statistical hypothesis tests, it has become standard practice to report the $p$-value (an abbreviation for probability value) of the test.

- If we have the p-value of a test, $p$, we can determine the outcome of the test by comparing the $p$-value to the chosen level of significance,$\alpha$, without looking up or calculating the critical values.

- This is much more convenient

---

## The p-Value Rule

- $p \leq \alpha$, Reject $H_0$

- $p > \alpha$, do not Reject $H_0$


## Example 3.3 (Continued)

- $H0: β2 ≤ 5.5$ vs $H1: β2 > 5.5$

- The test statistic: $t= \frac{b2-c}{se(b2)} = \frac{10.21-5.5}{2.09}= 2.25$

- The p-value is: $p = P[t_{(38)}\ge 2.25]=1-P[t_{(38)}\leq 2.25] = 1-0.9848=0.0152$

- $\alpha=0.01$,  do not reject $H_0$ 
- Do you reject/not reject $H_0$ if $\alpha = 0.05?$ 

---

```{r,echo=TRUE}
# Example 3.3
summary(glht(fit, linfct = c("income <= 5.5")))

```

---

## Example 3.4 (Continued)

- The null hypothesis is $H0: β2 \ge 15$ vs $H1: β2 < 15$

- The value of the test statistic is: $t= \frac{b2-c}{se(b2)} = \frac{10.21-15}{2.09}= - 2.29$

- The p-value is: $p = P[t_{(38)}\leq -2.29]= 0.0139$

- If $\alpha=0.01$, we do not reject $H_0$.

---

```{r,echo=TRUE}
# Example 3.4
summary(glht(fit, linfct = c("income >= 15")))

```

---

## Example 3.5 (Continued)

- The null hypothesis is $H0: β2 = 7.5$ vs $H1: β2  \neq 7.5$

- The value of the test statistic is: $t= 1.29$

- The p-value is: $p = P[t_{(38)}\ge 1.29]+P[t_{(38)}\leq -1.29]= 0.2033$

- Do not  reject $H_0$ for $\alpha=0.05, 0.10$ or $\alpha=0.20$ b/c $p > \alpha$.

---

```{r,echo=TRUE}
# Example 3.5
summary(glht(fit, linfct = c("income = 7.5")))
```

---

## Example 3.6 (Continued)

- $H0: β2 = 0$ vs $H1: β2  \neq 0$

- The test statistic: $t= 4.88$

- The p-value is: $p = P[t_{(38)}\ge 4.88]+P[t_{(38)}\leq -4.88]= 0.000$

---

```{r,echo=TRUE}
# Example 3.6
summary(fit)  # Includes two-tail test for slope
confint(fit, level=0.95)  # CI for slope
```

---

## Linear Combinations of Parameters

- We may wish to estimate and test hypotheses about a linear combination of parameters, 

$$\lambda = c_1 \beta_1 + c_2 \beta_2$$, where $c1$ and $c_2$ are constants that we specify


- Under SR1–SR5, $$\hat{\lambda} = c_1 b_1 + c_2 b_2$$ is the best linear unbiased estimator of $\lambda$.

---

## Linear...

- One example is if we wish to estimate the expected value $E(y|x)$, 

- when $x$ takes some specific value, such as $x=x_0$

- In this case, $c_1 = 1$ & $c_2 = x_0$, so that, 

$$\lambda = c_1\beta_1 +c_2\beta_2= \beta_1 +x_0\beta_2=E(y|x=x_0)$$

- Under assumption SR1-SR5, since the OLS estimators $b_1$ and $b_2$ are BLUE of $\beta_1$ and $\beta_2$,

---

## Linear...

- $\hat{\lambda} = c_1 b_1 + c_2 b_2$ is unbiased estimator of $\lambda = c_1 \beta_1 + c_2 \beta_2$ because: 


$$E(\hat \lambda|x)=E(c_1b_1 | x) + E(c_2b_2 | x)$$
$$= c_1E(b_1 | x) + c_2E(b_2 | x) $$
$$= c_1\beta_1 + c_2\beta_2 = \lambda$$

---

## Linear...

- The variance of $\hat \lambda$ is

$$\text{var} (\hat \lambda | x) = \text{var}(c_1b_1 + c_2b_2|\mathbf x) $$
$$= c_1^2 \text{var}(b_1|\mathbf x) + c_2^2 \text{var}(b_2|\mathbf x) + 2c_1c_2 \text{cov}(b_1, b_2|\mathbf x)$$

- The standard error:

$$se(\hat \lambda) = \sqrt var(\hat \lambda|x)= \sqrt (\hat{\text{var}}(c_1b_1 + c_2b_2)$$


---

## Eg 3.7. Estimating expected food expenditure 

- Estimate the average food weekly food expenditure by hhs with $\$ 2,000$ weekly income.

$$E \text{(Food_exp|Income)}=\beta_1 +\beta_2* \text{Income} $$
- If Income $=20$, the average food expenditure will be

$$E (\text{Food_exp|Income}=20)=\beta_1 +\beta_2 *20 $$

---


## Eg. 3.7 ...

- The estimated model: $$\widehat{Food\_exp} = 83.4160 +10.2096 *\text{Income}$$

- The point estimate of the average weekly food expenditure for a HH with $\$ 2,000$ income is:

$$E (\text{Food_exp|Income}=20)=\widehat{Food\_exp}$$ 
$$= 83.4160 +10.2096 *20=287.6089$$

---

## Eg. 3.8 Interval estimate of expected food expenditure.


- A $100(1-\alpha)\%$ interval estimate for $c_1\beta_1 +c_2\beta_2$ is:

$$ (c_1b_1 + c_2b_2)\pm t_c \text{se}(c_1b_1 + c_2b_2) $$
- Using our example,a $100(1-\alpha)\%$ interval estimate is given by:

$$(b_1+b_2 *20)\pm t_c \text{se}(b_1 +b_2 *20)$$


---

```{r,echo=TRUE}
# Example: Predict expected value at income=20
predict(fit, newdata = data.frame(income=20), interval="confidence")

```

```{r,echo=TRUE}
library(car)
dmt <- deltaMethod(fit, "b1 + 20*b2 ", parameterNames = c("b1", "b2"))
dmt 

```

---

## Testing a linear combination of parameters

- A general linear hypothesis involves both parameters, $\beta_1$ and $\beta_2$ and may be stated as:

$$H_0:(c_1\beta_1 +c_2 \beta_2)=c_0 $$

Or, equivalently

$$H_0:(c_1\beta_1 +c_2 \beta_2)-c_0=0 $$

---

## Alternative hypotheses

- The alternative hypothesis might be any one of the following:

$$
\begin{aligned}
&\text{(i) } H_1: c_1\beta_1 + c_2\beta_2 \neq c_0 \quad \text{(two-tail test)} \\
&\text{(ii) } H_1: c_1\beta_1 + c_2\beta_2 > c_0 \quad \text{(right-tail test)} \\
&\text{(iii) } H_1: c_1\beta_1 + c_2\beta_2 < c_0 \quad \text{(left-tail test)}
\end{aligned}
$$


- The t-statistic is: 
$$t =\frac{\hat \lambda-\lambda}{se(\hat{\lambda})}= \frac{(c_1b_1 + c_2b2_2) - c_0}{\text{se}(c_1b1_1 + c_2b2_2)} \sim t_{(N-2)}$$
---

## Testing...


- The rejection regions for the one- and two-tail alternatives (i) – (iii) are the same as those described in Section 3.3, and conclusions are interpreted the same way as well



## Testing a linear combination



 - Test $H_0: b1 + 20b2 <= 250$ vs $H_1: b1 + 20b2 > 250$
 
 
```{r,echo=TRUE}
# Example 3.9
library(car)
dmt <- deltaMethod(fit, "b1 + 20*b2 - 250", parameterNames = c("b1", "b2"))
dmt
```


```{r,echo=TRUE}
t_val <- dmt$Estimate / dmt$SE # the test-value 
t_val 
```


```{r,echo=TRUE}
# compare the test-value with the critical value 
t_val > qt(0.95, 38)  # TRUE, hence reject H0
```

---





  